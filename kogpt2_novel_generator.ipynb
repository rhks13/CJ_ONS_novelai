{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcd07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\cj\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\cj\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.12->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from packaging->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\cj\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\cj\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\cj\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\cj\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724ed621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "     -------------------------------------- 462.8/462.8 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "     ---------------------------------------- 20.6/20.6 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (0.12.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\cj\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\cj\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ---------------------------------------- 110.5/110.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cj\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed datasets-2.9.0 dill-0.3.6 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6aa521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac06889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad7fff358334279924c7785b91ad4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5657d2b1c0ef4ab1b8ce9c2dd6d976b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁안녕',\n",
       " '하',\n",
       " '세',\n",
       " '요.',\n",
       " '▁한국어',\n",
       " '▁G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '▁입',\n",
       " '니다.',\n",
       " '😤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "bos_token='</s>', eos_token='</s>', unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5e9ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af761eb23b4c229944ffa2d0212787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지친 몸을 이끌고 그는 다시 한 번 자신의 길을 걸었다.\n",
      "그가 떠난 곳은 바로 그 유명한 '파란만장'이었다.\n",
      "그는 이제 막 걸음마를 시작한 그의 발걸음을 따라갔다.\n",
      "그의 발자국 소리가 들리는 순간, 그가 걸어가는 모습이 보였다.\n",
      "\"이제 곧 도착할 거야.\"\n",
      "아직까지 그를 기다리고 있던 사람은 아무도 없었다.\n",
      "하지만 그녀는 이미 그에게 다가온 사람처럼 천천히 걷기 시작했다.\n",
      "그리고는 이내 그녀의 뒤를 따랐다.\n",
      "그러자 그녀가 멈춰 섰다.\n",
      "잠시 후 아까의 일이 생각났다.\n",
      "바로 그때였다.\n",
      "어느새 그녀도 그녀를 향해 달려가고 있었다.\n",
      "그러나 그것은 잠시뿐이었다.\n",
      "다시 한번 그녀에게 다가가려는 듯했다.\n",
      "마치 자신이 지금 이 자리에 서 있는 것 같은 기분이 들었다.\n",
      "갑자기 나타난 것은 다름 아닌 '아무것도 아니다!'\n",
      "순간적으로 느껴지는 두려움. 그리고 그것이 자신을 향한 분노로 느껴졌다.\n",
      "자신의 행동이 자신에게 어떤 영향을 미칠지 알 수 없는 일이었다.\n",
      "자신이 왜 이렇게 되었는지.\n",
      "지금껏 알고 있었던 것을 모두 잊고 그저 멍하니 앉아 있을 뿐이었다.\n",
      "한참을 그렇게 기다린 후에야 비로소 깨달았다.\n",
      "마지막으로 남은 시간은 단 하루뿐인 것이다.\n",
      "그래서인지 오늘은 더 이상 아무것도 할 수가 없다.\n",
      "더 이상은 아무 것도 하지 않기로 했다.\n",
      "대신 조금이라도 빨리 일어나려고 노력했다.\n",
      "물론 아직까지는 아무런 행동 없이 그냥 지나칠 수밖에 없었지만, 그래도 여전히 무언가를 하고 싶다는 생각이 들어 더욱 열심히 움직이고 있다.\n",
      "오늘따라 유난히 추웠던 날씨에 온몸에서 땀방울들이 흘러내리고 있었고, 어느덧 차가운 바람이 불어와 마치 얼음 위를 걷는 듯한 착각이 일기도 했지만, 그런 생각은 전혀 들지 않았다.\n",
      "오히려 오히려 마음이 편해졌다.\n",
      "뭔가에 열중하고 있다는 느낌이 들어서다.\n",
      "혹시라도 누군가의 도움을 받을 수도 있지 않을까 하는 생각에 가슴이 두근거렸다.\n",
      "사실 나는 내일이면 또 다른 사람이 나타날지도 모른다는 생각을 해본 적이 거의 없었기 때문이다.\n",
      "나는 정말이지 내가 무슨 일을 해야 할지 몰랐다.\n",
      "내가 무엇을 하든 상관없지만 말이다.\n",
      "내일은 꼭 나 혼자만의 시간이 될 테니까.\n",
      "나 역시 마찬가지이다.\n",
      "당연히 나도 당분간은 이런 식으로 지내야만 한다.\n",
      "왜냐하면 난 언제나 나를 위해 모든 것들을 희생해야 하기 때문이기도 하다.\n",
      "난 항상 나의 곁에 있어주고 있기 때문에\n",
      "나의 존재는 나에게 있어서 가장 소중한 존재이기 때문이다. 만약 그렇다면 어떻게 하면 좋을까?\n",
      "우선 먼저 내게 필요한 것이 무엇인지부터 생각해보자.\n",
      "먼저 당신의 존재를 알아보는 일이다.\n",
      "또한, 우리가 함께 살아가면서 겪게 되는 여러 가지 일들을 떠올려볼 필요가 있겠고, 또한 우리의 관계를 돌아보고 새로운 관계들을 만들어나가는 데 도움이 되리라 생각한다.\n",
      "둘째, 우리를 둘러싼 환경을 살펴보아야겠다. 만일 이러한 환경 속에서 살아가는 사람들이 있다면 우리는 서로에게 많은 상처를 줄 것이기 때문에, 그들을\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = '지친 몸을 이끌고 그는'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=512,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf5a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('romance.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e844595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def labeling(string):\n",
    "    return \" \".join(string.split('.')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9208664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['labels'] = df['context'].map(labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "649e3626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“도대체 무슨 술수를 부린 거야.”\\n승전 연회의 마지막 날이었다. 날카로운 말이 ...</td>\n",
       "      <td>“도대체 무슨 술수를 부린 거야 ”\\n승전 연회의 마지막 날이었다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마차가 도착한 곳은 아름답고 웅장한 저택이었다.\\n\\n올리비아는 저택을 올려다보았다...</td>\n",
       "      <td>마차가 도착한 곳은 아름답고 웅장한 저택이었다 \\n\\n올리비아는 저택을 올려다보았다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>모든 게 달라졌다.\\n\\n유모는 돌아오지 않았고 늘 빵과 고기, 신선한 채소와 과일...</td>\n",
       "      <td>모든 게 달라졌다 \\n\\n유모는 돌아오지 않았고 늘 빵과 고기, 신선한 채소와 과일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오늘도 안 오시려나.\\n\\n올리비아는 한숨을 삼켰다. 차는 식은 지 오래였다. 벌써...</td>\n",
       "      <td>오늘도 안 오시려나 \\n\\n올리비아는 한숨을 삼켰다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“그러면 다음 주 월요일에도 태자 전하를 뵈러 오시는 거예요?”\\n마차가 궁 밖으로...</td>\n",
       "      <td>“그러면 다음 주 월요일에도 태자 전하를 뵈러 오시는 거예요?”\\n마차가 궁 밖으로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>* * *\\n“공…… 흡!”\\n레아는 저도 모르게 공작님이라고 부를 뻔한 입을 두 ...</td>\n",
       "      <td>* * *\\n“공…… 흡!”\\n레아는 저도 모르게 공작님이라고 부를 뻔한 입을 두 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>* * *\\n\\n\\n\\n테오를 따라온 방 앞에 선 악키아가 안쪽을 훑었다. 오래된 ...</td>\n",
       "      <td>* * *\\n\\n\\n\\n테오를 따라온 방 앞에 선 악키아가 안쪽을 훑었다  오래된 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>* * *\\n\\n\\n\\n레아 역시 잠이 오지 않았는지, 숄을 걸친 채 그를 바라보고...</td>\n",
       "      <td>* * *\\n\\n\\n\\n레아 역시 잠이 오지 않았는지, 숄을 걸친 채 그를 바라보고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>* * *\\n\\n\\n\\n하지만 레아의 생각과는 반대로 악키아 역시 퍽 난감해하고 있...</td>\n",
       "      <td>* * *\\n\\n\\n\\n하지만 레아의 생각과는 반대로 악키아 역시 퍽 난감해하고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>* * *\\n“눈 떠.”\\n레아가 두 눈을 뜨자, 금방이라도 코가 닿을 것처럼 가까...</td>\n",
       "      <td>* * *\\n“눈 떠 ”\\n레아가 두 눈을 뜨자, 금방이라도 코가 닿을 것처럼 가까...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "0     “도대체 무슨 술수를 부린 거야.”\\n승전 연회의 마지막 날이었다. 날카로운 말이 ...   \n",
       "1     마차가 도착한 곳은 아름답고 웅장한 저택이었다.\\n\\n올리비아는 저택을 올려다보았다...   \n",
       "2     모든 게 달라졌다.\\n\\n유모는 돌아오지 않았고 늘 빵과 고기, 신선한 채소와 과일...   \n",
       "3     오늘도 안 오시려나.\\n\\n올리비아는 한숨을 삼켰다. 차는 식은 지 오래였다. 벌써...   \n",
       "4     “그러면 다음 주 월요일에도 태자 전하를 뵈러 오시는 거예요?”\\n마차가 궁 밖으로...   \n",
       "...                                                 ...   \n",
       "1695  * * *\\n“공…… 흡!”\\n레아는 저도 모르게 공작님이라고 부를 뻔한 입을 두 ...   \n",
       "1696  * * *\\n\\n\\n\\n테오를 따라온 방 앞에 선 악키아가 안쪽을 훑었다. 오래된 ...   \n",
       "1697  * * *\\n\\n\\n\\n레아 역시 잠이 오지 않았는지, 숄을 걸친 채 그를 바라보고...   \n",
       "1698  * * *\\n\\n\\n\\n하지만 레아의 생각과는 반대로 악키아 역시 퍽 난감해하고 있...   \n",
       "1699  * * *\\n“눈 떠.”\\n레아가 두 눈을 뜨자, 금방이라도 코가 닿을 것처럼 가까...   \n",
       "\n",
       "                                                 labels  \n",
       "0                  “도대체 무슨 술수를 부린 거야 ”\\n승전 연회의 마지막 날이었다  \n",
       "1        마차가 도착한 곳은 아름답고 웅장한 저택이었다 \\n\\n올리비아는 저택을 올려다보았다  \n",
       "2     모든 게 달라졌다 \\n\\n유모는 돌아오지 않았고 늘 빵과 고기, 신선한 채소와 과일...  \n",
       "3                          오늘도 안 오시려나 \\n\\n올리비아는 한숨을 삼켰다  \n",
       "4     “그러면 다음 주 월요일에도 태자 전하를 뵈러 오시는 거예요?”\\n마차가 궁 밖으로...  \n",
       "...                                                 ...  \n",
       "1695  * * *\\n“공…… 흡!”\\n레아는 저도 모르게 공작님이라고 부를 뻔한 입을 두 ...  \n",
       "1696  * * *\\n\\n\\n\\n테오를 따라온 방 앞에 선 악키아가 안쪽을 훑었다  오래된 ...  \n",
       "1697  * * *\\n\\n\\n\\n레아 역시 잠이 오지 않았는지, 숄을 걸친 채 그를 바라보고...  \n",
       "1698  * * *\\n\\n\\n\\n하지만 레아의 생각과는 반대로 악키아 역시 퍽 난감해하고 있...  \n",
       "1699  * * *\\n“눈 떠 ”\\n레아가 두 눈을 뜨자, 금방이라도 코가 닿을 것처럼 가까...  \n",
       "\n",
       "[1700 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a8385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩하는 과정\n",
    "from datasets import Dataset\n",
    "train_data = Dataset.from_pandas(df[:int(len(df)*0.8)])\n",
    "eval_data = Dataset.from_pandas(df[int(len(df)*0.8):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1002547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51977ced5d564c9a8af41dc7dae0c5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d13e17c6da46fc9209299e7cad54bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"context\"], truncation=True, max_length=512)\n",
    "\n",
    "train_token = train_data.map(tokenize_function, batched=True)\n",
    "eval_token = eval_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786d5573",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'mlm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21016\\1977088461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_collator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmlm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlm_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'mlm'"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870c6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9be24a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d8a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/config.json from cache at C:\\Users\\CJ/.cache\\huggingface\\transformers\\13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/skt/kogpt2-base-v2/resolve/main/pytorch_model.bin from cache at C:\\Users\\CJ/.cache\\huggingface\\transformers\\495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at skt/kogpt2-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea69b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_token,\n",
    "    eval_dataset=eval_token,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c53d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: context, __index_level_0__.\n",
      "***** Running training *****\n",
      "  Num examples = 1360\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 510\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21016\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1282\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1787\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1789\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1829\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m             \u001b[1;31m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1831\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m   1933\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1934\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1935\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1937\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d77c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910a800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0702a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
